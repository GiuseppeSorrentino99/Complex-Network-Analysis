{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NETWORK ANALYSIS: Les Miserables\n",
    "## Author: Giuseppe Sorrentino 10864176\n",
    "# THE TASK\n",
    "The aim of the present analysis is to analyze the network of characters in \"Les Miserable\", written by Victor Hugo, trying to find the main topology indicators.\n",
    "Thanks to this, it is possible to analyze characters and their centrality, degree and so on, verifying their importance with the real story.\n",
    "\n",
    "The present analysis is mainly based on **NetworkX**, an open source library to analyze graphs in different formats, like GML or GEXF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import namedtuple,defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import math\n",
    "import random as rd\n",
    "import operator\n",
    "from networkx.algorithms.community import naive_greedy_modularity_communities\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**structure**\n",
    "\n",
    "The following lines of code allows to verify the structure of the network.\n",
    "\n",
    "Being interactions between characters, it is reasonable to think that the network is undirected.\n",
    "\n",
    "The main questions for this analysis are:\n",
    "- what is the number of nodes?\n",
    "- what is the number of edges?\n",
    "\n",
    "All these data are required to fully understand the topology analysis provided later in the present work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = nx.read_gml(\"lesmiserables.gml\")\n",
    "\n",
    "no_nodes = g.number_of_nodes()\n",
    "no_edges = g.number_of_edges()\n",
    "\n",
    "print(\"number of nodes:\" + str (no_nodes))\n",
    "print(\"number of edges:\" + str (no_edges))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is now to look for:<br>\n",
    "\n",
    "1. Degree medio <br>\n",
    "2. Degree centrality <br>\n",
    "3. Betweenness centrality <br>\n",
    "4. Closeness centrality <br>\n",
    "5. Eigenvector centrality <br>\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = nx.degree(g)\n",
    "degrees_values = []\n",
    "for node, deg in degrees:\n",
    "    degrees_values.append(deg)\n",
    "mean_degree = np.mean(degrees_values)\n",
    "print(\"il grado medio della rete sarà : \" + str(mean_degree))\n",
    "degree_centralities = nx.degree_centrality(g)\n",
    "sorted_nodes = dict(sorted(degree_centralities.items(), key=operator.itemgetter(1),reverse=True))\n",
    "sorted_nodes = list(sorted_nodes.keys())\n",
    "print(\"i 5 nodi con la degree centrality più alta sono:\")\n",
    "print(sorted_nodes[0])\n",
    "print(sorted_nodes[1])\n",
    "print(sorted_nodes[2])\n",
    "print(sorted_nodes[3])\n",
    "print(sorted_nodes[4])\n",
    "betweennes_centralities = nx.betweenness_centrality(g)\n",
    "sorted_nodes = dict(sorted(betweennes_centralities.items(), key=operator.itemgetter(1),reverse=True))\n",
    "sorted_nodes = list(sorted_nodes.keys())\n",
    "print(\"i 5 nodi con la betweennes centrality più alta sono:\")\n",
    "print(sorted_nodes[0])\n",
    "print(sorted_nodes[1])\n",
    "print(sorted_nodes[2])\n",
    "print(sorted_nodes[3])\n",
    "print(sorted_nodes[4])\n",
    "closeness_centralities = nx.closeness_centrality(g)\n",
    "sorted_nodes = dict( sorted(closeness_centralities.items(), key=operator.itemgetter(1),reverse=True))\n",
    "sorted_nodes = list(sorted_nodes.keys())\n",
    "print(\"i 5 nodi con la closeness centrality più alta sono:\")\n",
    "print(sorted_nodes[0])\n",
    "print(sorted_nodes[1])\n",
    "print(sorted_nodes[2])\n",
    "print(sorted_nodes[3])\n",
    "print(sorted_nodes[4])\n",
    "eigenvector_centralities = nx.eigenvector_centrality(g)\n",
    "sorted_nodes = dict(sorted(eigenvector_centralities.items(), key=operator.itemgetter(1),reverse=True))\n",
    "sorted_nodes = list(sorted_nodes.keys())\n",
    "print(\"i 5 nodi con la eigenvector centrality più alta sono:\")\n",
    "print(sorted_nodes[0])\n",
    "print(sorted_nodes[1])\n",
    "print(sorted_nodes[2])\n",
    "print(sorted_nodes[3])\n",
    "print(sorted_nodes[4])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now setup a core-periphery analysis using K-core decomposition\n",
    "\n",
    "The idea is following all the steps of K-core decomposition.\n",
    "1. we set K = 1\n",
    "2. all nodes with K = 1 are inserted in the K-shell and removed from the network.\n",
    "3. if there are still nodes with grade = K, they are inserted in the K-shell\n",
    "4. I repeat step 2-3 up to no new nodes are added. Once this happens, I increase K and repeat from step 1\n",
    "the algorithm runs until all nodes are inserted in a core.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = []\n",
    "i = 0\n",
    "copied_core = nx.read_gml(\"lesmiserables.gml\")\n",
    "dim = copied_core.number_of_nodes()\n",
    "flag = True\n",
    "while(copied_core.number_of_nodes() > 0 ):\n",
    "    if(flag):\n",
    "        lista = []\n",
    "    degree_centralities = nx.degree_centrality(copied_core)\n",
    "    labels = dict(sorted(degree_centralities.items(), key=operator.itemgetter(1),reverse=False))\n",
    "    labels = list(labels.keys())\n",
    "    flag = True\n",
    "    for j in range(0,len(labels),1):\n",
    "        val = copied_core.degree(labels[j])\n",
    "        if (val <= i):\n",
    "            lista.append(labels[j])\n",
    "            copied_core.remove_node(labels[j])\n",
    "            flag = False\n",
    "    if(flag):\n",
    "        i = i+1\n",
    "        cores.append(lista)\n",
    "print(\"il valore di K è: \"+ str(len(cores)-1))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What happens under attack!\n",
    "The idea in this section is to analyze what happens removing nodes.\n",
    "\n",
    "It essentially simulates attacks or random failures.\n",
    "\n",
    "In a real network of devices, for example, this would allow to evaluate what happens in case of attacks or randomic problems. The idea is that a random failure removes a node randomly while the attacks remove the one with highest centrality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.number_connected_components(g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As abvious, the network is connected so now we have just one components."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's consider **random** attacks. The idea here is:\n",
    "1. Let's pick a random number R\n",
    "2. Let's pick the label L of the node R from the list of nodes.\n",
    "3. Let's remove the node labelled L\n",
    "4. Let's look for all the conncted components, ordered by len\n",
    "5. Let's pick the biggest one, and let's repeat the algorithm until the research destroys the network. In this case, it is possible to set a threshold. The used here is 10%, so we conclude the algorithm once the biggest component has less than 10% of nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter = []\n",
    "#t = 1/dim\n",
    "t = 0.1\n",
    "threshold = dim * t\n",
    "threshold_string = str(t*100) + \"%\" + \" dei nodi, ovvero \" + str(math.floor(threshold)) + \" nodi\"\n",
    "print(\"vediamo quanti nodi devo rimuovere per avere la componente gigante con meno del \" + threshold_string)\n",
    "#Visto che la rete è randomica, per avere dei valori significativi devo ripetere più volte l'esperimento\n",
    "for k in range(1,100,1):\n",
    "    g = nx.read_gml(\"lesmiserables.gml\")\n",
    "    count = 0\n",
    "    flag = 0\n",
    "    dim = g.number_of_nodes()\n",
    "    for i in range(dim):\n",
    "        #print(\"----------------------------------------------------------------------------------------------------------------------------\")\n",
    "        n = g.number_of_nodes()\n",
    "        rand = rd.randint(1,n-1)\n",
    "        labels = list(g.nodes())\n",
    "        rem = labels[rand]\n",
    "        g.remove_node(rem)\n",
    "        count= count +1\n",
    "        Gcc = sorted(nx.connected_components(g), key=len, reverse=True)\n",
    "        giant = g.subgraph(Gcc[0])\n",
    "        #for j in range(len(Gcc)):\n",
    "        #    print(len(Gcc[j]))\n",
    "        if giant.number_of_nodes() <= threshold:\n",
    "            break\n",
    "    perc_rimossi = math.floor(count/dim * 100)\n",
    "    iter.append(perc_rimossi)\n",
    "iter_array = np.array(iter)\n",
    "mean = round(np.mean(iter_array))\n",
    "perc_rimossi_string = str(mean) +\"%\"\n",
    "print(\"la percentuale di nodi rimossi a causa dei failure è:\" + perc_rimossi_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.number_connected_components(g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's consider an **attack**. More specifically, <br>\n",
    "1. Let's compute the information centrality of each node.\n",
    "2. Than we can pick the most central node, and we can remove it from the network.\n",
    "3. At every step, we compute the information centrality again, and we remove the most central node\n",
    "4. Once the threshold exceeds the dimension of the giant component of the network, we the algorithms ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#devo ora simulare degli attacchi mirati, quindi prendo essenzialmente i nodi con betweenness più alta. Quelli saranno i nodi da rimuovere\n",
    "copied_g = nx.read_gml(\"lesmiserables.gml\")\n",
    "information_centralities = nx.information_centrality(copied_g)\n",
    "sorted_nodes = dict( sorted(information_centralities.items(), key=operator.itemgetter(1),reverse=True))\n",
    "sorted_nodes=list(sorted_nodes.keys())\n",
    "y = nx.number_connected_components(copied_g)\n",
    "count_attack = 0\n",
    "flag_attack = 0\n",
    "dim_attack = copied_g.number_of_nodes()\n",
    "#t = 1/dim\n",
    "t = 0.1\n",
    "threshold_attack = dim_attack * t\n",
    "threshold_string_attack = str(int(t*100)) + \"%\" + \" dei nodi, ovvero \" + str(math.floor(threshold_attack)) + \" nodi\"\n",
    "print(\"vediamo quanti nodi devo attaccare per avere la componente gigante con meno del \" + threshold_string_attack)\n",
    "rem_attack = sorted_nodes[0]\n",
    "for i in sorted_nodes:\n",
    "    copied_g.remove_node(rem_attack)\n",
    "    count_attack= count_attack +1\n",
    "    Gcc_attack = sorted(nx.connected_components(copied_g), key=len, reverse=True)\n",
    "    # for j in range(len(Gcc_attack)):\n",
    "    #       print(len(Gcc_attack[j]))\n",
    "    giant_attack = copied_g.subgraph(Gcc_attack[0])\n",
    "    dimensione_giant_attack = giant_attack.number_of_nodes()\n",
    "    if dimensione_giant_attack <= threshold_attack:\n",
    "        break\n",
    "    information_centralities = nx.information_centrality(giant_attack)\n",
    "    sorted_nodes = dict( sorted(information_centralities.items(), key=operator.itemgetter(1),reverse=True))\n",
    "    resorted=list(sorted_nodes.keys())\n",
    "    #print(resorted)\n",
    "    rem_attack = resorted[0]\n",
    "    # print(sorted_nodes.get(rem_attack))\n",
    "    #print(rem_attack)\n",
    "\n",
    "print(\"ho rimosso: \" + str(count_attack) + \" nodi\")\n",
    "perc_rimossi = math.floor(count_attack/dim_attack * 100)\n",
    "perc_rimossi_string = str(perc_rimossi) + \"%\"\n",
    "print(\"la percentuale di nodi rimossi nell'attacco è:\" + perc_rimossi_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64924d34f5c95f261818ee0e41d58b43741dcf0b72677c10b726c1fe86046c0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
